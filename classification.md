|                     **Method**                      |  **Backbone**   |  **Year**   | **Top1-acc(\%)** | **Top5-acc(\%)** | **Parameter(M)** |
| :-------------------------------------------------: | :-------------: | :---------: | :--------------: | :--------------: | :--------------: |
|     **AlexNet**<sup><a href="#ref1">1</a></sup>     |     AlexNet     |  NIPS 2012  |       63.3       |       84.6       |        60        |
|     **VGG-16**<sup><a href="#ref2">2</a></sup>      |     VGG-16      |  ICLR 2015  |       74.4       |       91.9       |       138        |
|     **VGG-19**<sup><a href="#ref2">2</a></sup>      |     VGG-19      |  ICLR 2015  |       74.5       |       92.0       |       144        |
|    **ResNet-50**<sup><a href="#ref3">3</a></sup>    |     ResNet      |  CVPR 2016  |      77.15       |      93.29       |        25        |
|   **ResNet-152**<sup><a href="#ref3">3</a></sup>    |     ResNet      |  CVPR 2016  |       81.3       |       95.4       |        \-        |
|  **Inception V2**<sup><a href="#ref4">4</a></sup>   |  InceptionNet   |  PMLR 2015  |       74.8       |       92.2       |       11.2       |
|  **MobileNet-224**<sup><a href="#ref5">5</a></sup>  |    MobileNet    | arXiv 2017  |       70.6       |       89.5       |        \-        |
|  **DenseNet-121**<sup><a href="#ref6">6</a></sup>   |    DenseNet     |  CVPR 2017  |      74.98       |      92.29       |        \-        |
|  **DenseNet-264**<sup><a href="#ref6">6</a></sup>   |    DenseNet     |  CVPR 2017  |      77.85       |      93.88       |        \-        |
|    **Xception**<sup><a href="#ref7">7</a></sup>     |    Xception     |  CVPR 2017  |        79        |       94.5       |       22.8       |
|  **Inception V3**<sup><a href="#ref8">8</a></sup>   |  InceptionNet   |  CVPR 2018  |      77.12       |        \-        |        \-        |
| **EfficientNet-B0**<sup><a href="#ref9">9</a></sup> |  EfficientNet   |  ICML 2019  |       76.3       |       93.2       |       5.3        |
| **EfficientNet-B2**<sup><a href="#ref9">9</a></sup> |  EfficientNet   |  ICML 2019  |       79.8       |       94.9       |       9.2        |
| **EfficientNet-B4**<sup><a href="#ref9">9</a></sup> |  EfficientNet   |  ICML 2019  |       82.6       |       96.3       |        19        |
| **EfficientNet-B7**<sup><a href="#ref9">9</a></sup> |  EfficientNet   |  ICML 2019  |       84.4       |       97.1       |        66        |
|      **BBG**<sup><a href="#ref10">10</a></sup>      |     ResNet      | ICASSP 2020 |       62.6       |       84.1       |        \-        |
|  **DenseNAS-R1**<sup><a href="#ref11">11</a></sup>  |     ResNet      |  CVPR 2020  |       73.5       |        \-        |       11.1       |
|  **DenseNAS-R3**<sup><a href="#ref11">11</a></sup>  |     ResNet      |  CVPR 2020  |       78.0       |        \-        |       24.7       |
|   **MUXNet-xs**<sup><a href="#ref12">12</a></sup>   |     MUXNet      |  CVPR 2020  |       66.7       |       86.8       |       1.8        |
|   **MUXNet-l**<sup><a href="#ref12">12</a></sup>    |     MUXNet      |  CVPR 2020  |       76.6       |       93.2       |       4.0        |
| **NoisyStudent** <sup><a href="#ref13">13</a></sup> | EfficientNet-B2 |  CVPR 2020  |       82.4       |       96.3       |       9.2        |
|   **ECA-Net** <sup><a href="#ref14">14</a></sup>    |     ResNet      |  CVPR 2020  |      78.65       |      94.34       |      42.49       |
| **ResNeSt-101** <sup><a href="#ref15">15</a></sup>  |     ResNeSt     | CVPR 2022  |       83.0       |        \-        |        \-        |
| **ResNeSt-269** <sup><a href="#ref15">15</a></sup>  |     ResNeSt     | CVPR 2022  |       84.5       |        \-        |        \-        |
| **RepMLP-Res50** <sup><a href="#ref16">16</a></sup> |     ResNet      | arXiv 2021  |      78.60       |        \-        |        40        |
|   **ViT-G/14**<sup><a href="#ref17">17</a></sup>    |       ViT       | CVPR 2021  |      90.45       |        \-        |       1843       |
|   **ViT-H/14**<sup><a href="#ref18">18</a></sup>    |       VIT       |  ICLR 2021  |      88.08       |        \-        |       656        |
|      **MAE**<sup><a href="#ref19">19</a></sup>      |      ViT-H      | CVPR 2022  |       86.9       |        \-        |        \-        |
|      **MAE**<sup><a href="#ref19">19</a></sup>      |      ViT-B      | CVPR 2022  |       83.6       |        \-        |        \-        |
|   **SwinV1-B**<sup><a href="#ref20">20</a></sup>    |      Swin       |  CVPR 2021  |        86        |        \-        |        88        |
|   **SwinV1-L** <sup><a href="#ref20">20</a></sup>   |      Swin       |  CVPR 2021  |       87.3       |        \-        |       197        |
|   **SwinV2-B**<sup><a href="#ref21">21</a></sup>    |      Swin       | CVPR 2022  |       87.1       |        \-        |        88        |
|   **SwinV2-G** <sup><a href="#ref21">21</a></sup>   |      Swin       |  CVPR 2022  |      90.17       |        \-        |       300        |
|    **BEiT-B** <sup><a href="#ref22">22</a></sup>    |       ViT       | arXiv 2022  |       86.3       |        \-        |        86        |
|    **BEiT-L**<sup><a href="#ref22">22</a></sup>     |       ViT       | arXiv 2022  |       88.6       |      98.66       |       331        |
|  **Model Soups**<sup><a href="#ref23">23</a></sup>  |       ViT       | ICLR 2022  |      90.94       |        \-        |       1843       |

1. <p name = "ref1">Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[J]. Communications of the ACM, 2017, 60(6): 84-90.</p>
2.  <p name = "ref2">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</p>
3.  <p name = "ref3">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</p>
4.  <p name = "ref4">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International conference on machine learning. pmlr, 2015: 448-456.</p>
5.  <p name = "ref5">Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017.</p>
6.  <p name = "ref6">Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708.</p>
7.  <p name = "ref7">Chollet F. Xception: Deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1251-1258.</p>
8.  <p name = "ref8">Palacio S, Folz J, Hees J, et al. What do deep networks like to see?[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3108-3117.</p>
9.  <p name = "ref9">Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]//International conference on machine learning. PMLR, 2019: 6105-6114.</p>
10. <p name = "ref10">Shen M, Liu X, Gong R, et al. Balanced binary neural networks with gated residual[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 4197-4201.</p>
11. <p name = "ref11">Fang J, Sun Y, Zhang Q, et al. Densely connected search space for more flexible neural architecture search[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 10628-10637.</p>
12. <p name = "ref12">Lu Z, Deb K, Boddeti V N. MUXConv: Information multiplexing in convolutional neural networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 12044-12053.</p>
13. <p name = "ref13">Xie Q, Luong M T, Hovy E, et al. Self-training with noisy student improves imagenet classification[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 10687-10698.</p>
14. <p name = "ref14">Wang Q, Wu B, Zhu P, et al. ECA-Net: Efficient channel attention for deep convolutional neural networks[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 11534-11542.</p>
15. <p name = "ref15">Zhang H, Wu C, Zhang Z, et al. Resnest: Split-attention networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 2736-2746.</p>
16. <p name = "ref16">Ding X, Xia C, Zhang X, et al. Repmlp: Re-parameterizing convolutions into fully-connected layers for image recognition[J]. arXiv preprint arXiv:2105.01883, 2021.</p>
17. <p name = "ref17">Fan H, Xiong B, Mangalam K, et al. Multiscale vision transformers[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 6824-6835.</p>
18. <p name = "ref18">Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.</p>
19. <p name = "ref19">He K, Chen X, Xie S, et al. Masked autoencoders are scalable vision learners[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16000-16009.</p>
20. <p name = "ref20">Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.</p>
21. <p name = "ref21">Liu Z, Hu H, Lin Y, et al. Swin transformer v2: Scaling up capacity and resolution[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 12009-12019.</p>
22. <p name = "ref22">Bao H, Dong L, Piao S, et al. Beit: Bert pre-training of image transformers[J]. arXiv preprint arXiv:2106.08254, 2021.</p>
23. <p name = "ref23">Wortsman M, Ilharco G, Gadre S Y, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time[C]//International Conference on Machine Learning. PMLR, 2022: 23965-23998.</p>