|                              **Method**                               |                            **Backbone**                            | **Publish Year** | **PASCAL VOC 2012 test\(%\)** | **ADE20K\(%\)** | **Cityscapes test\(%\)** |
| :-------------------------------------------------------------------: | :----------------------------------------------------------------: | :--------------: | :---------------------------: | :-------------: | :----------------------: |
|                                                                       |                                                                    |                  |             mIoU              |      mIoU       |           mIoU           |
|                **FCN**<sup><a href="#ref1">1</a></sup>                |                              VGG\-16                               |    CVPR 2015     |             62\.2             |     29\.39      |            \-            |
|              **BoxSup**<sup><a href="#ref2">2</a></sup>               |                              VGG\-16                               |    ICCV 2015     |             63\.8             |       \-        |            \-            |
|             **CRF\-RNN**<sup><a href="#ref3">3</a></sup>              |                                 \-                                 |    ICCV 2015     |             74\.7             |       \-        |          62\.5           |
|           **DeepLab\-CRF**<sup><a href="#ref4">4</a></sup>            |                              VGG\-16                               |    ICLR 2015     |             66\.4             |       \-        |          63\.1           |
|       **Dilated Convolutions**<sup><a href="#ref5">5</a></sup>        |                              VGG\-16                               |    ICLR 2016     |             67\.6             |     32\.31      |          67\.1           |
|              **SegNet**<sup><a href="#ref6">6</a></sup>               |                              VGG\-16                               |    TPAMI 2017    |              \-               |       \-        |          57\.0           |
|             **DeepLabV2**<sup><a href="#ref7">7</a></sup>             |                            ResNet\-101                             |    TPAMI 2017    |             79\.7             |       \-        |          70\.4           |
|          **DeepLabV3\-JFT**<sup><a href="#ref8">8</a></sup>           |                            ResNet\-101                             |    CVPR 2017     |             86\.9             |       \-        |          81\.3           |
|               **FRRN**<sup><a href="#ref9">9</a></sup>                |                               ResNet                               |    CVPR 2017     |              \-               |       \-        |          71\.8           |
|             **PSPNet**<sup><a href="#ref10">10</a></sup>              |                            ResNet\-101                             |    CVPR 2017     |             82\.6             |       \-        |          78\.4           |
|            **RefineNet**<sup><a href="#ref11">11</a></sup>            |                            ResNet\-101                             |    CVPR 2017     |              \-               |      40\.2      |          73\.6           |
|                **DFN**<sup><a href="#ref3">3</a></sup>                |                            ResNet\-101                             |    CVPR 2018     |             80\.6             |     43\.68      |          77\.8           |
|              **DSSPN**<sup><a href="#ref12">12</a></sup>              |                            ResNet\-101                             |    CVPR 2018     |              \-               |     43\.68      |          77\.8           |
|             **ESPNet**<sup><a href="#ref13">13</a></sup>              |                              VGG\-16                               |    ECCV 2018     |            63\.01             |       \-        |          60\.3           |
|             **UperNet**<sup><a href="#ref14">14</a></sup>             |                            ResNet\-101                             |    ECCV 2018     |              \-               |     42\.66      |            \-            |
|             **PSANet**<sup><a href="#ref15">15</a></sup>              |                            ResNet\-101                             |    ECCV 2018     |             85\.7             |     81\.51      |          78\.6           |
|        **DeepLabv3\-JFT\+**<sup><a href="#ref16">16</a></sup>         |                              Xception                              |    ECCV 2018     |             89\.0             |       \-        |          79\.6           |
|               **DPC**<sup><a href="#ref17">17</a></sup>               |                              Xception                              |   NeurIPS 2018   |             87\.9             |       \-        |          82\.7           |
|             **EDANet**<sup><a href="#ref18">18</a></sup>              |                              DenseNet                              |    ACMM 2019     |              \-               |       \-        |          67\.3           |
|               **SDN**<sup><a href="#ref19">19</a></sup>               |                           DenseNet                            |     TIP 2019     |             86\.6             |       \-        |            \-            |
|             **SVCNet**<sup><a href="#ref20">20</a></sup>              |                            ResNet\-101                             |    CVPR 2019     |              \-               |       \-        |          81\.0           |
|        **Auto-DeepLab(NAS)**<sup><a href="#ref21">21</a></sup>        |                            Auto-DeepLab                            |    CVPR 2019     |              \-               |       \-        |          81\.0           |
|            **ESPNetv2**<sup><a href="#ref22">22</a></sup>             |                            ResNet\-101                             |    CVPR 2019     |             68\.0             |       \-        |          66\.2           |
|              **HANet**<sup><a href="#ref23">23</a></sup>              |                            ResNet\-101                             |    CVPR 2020     |              \-               |       \-        |          82\.1           |
|              **CPNet**<sup><a href="#ref24">24</a></sup>              |                            ResNet\-101                             |    CVPR 2020     |              \-               |      46\.3      |          81\.3           |
|    **OCR\(Seg\. transformer\)**<sup><a href="#ref25">25</a></sup>     |                            ResNet\-101                             |    ECCV 2020     |              \-               |     45\.28      |          81\.8           |
|   **EfficientNet\-L2\+NAS\-FPN**<sup><a href="#ref25">25</a></sup>    |                          EfficientNet\-L2                          |   NeurIPS 2020   |             90\.5             |       \-        |            \-            |
|           **MaskFormer**<sup><a href="#ref26">26</a></sup>            |                            ResNet\-101                             |   NeurIPS 2021   |              \-               |      48\.1      |            \-            |
|           **MaskFormer**<sup><a href="#ref26">26</a></sup>            |                              Swin\-L                               |   NeurIPS 2021   |              \-               |      55\.6      |            \-            |
|           **DCNAS(NAS)**<sup><a href="#ref27">27</a></sup>            |                                 \-                                 |    CVPR 2021     |             86\.9             |     47\.12      |          83\.6           |
|           **DPT\-Hybrid**<sup><a href="#ref28">28</a></sup>           |                                ViT                                 |    ICCV 2021     |              \-               |     49\.02      |            \-            |
|             **HamNet**<sup><a href="#ref29">29</a></sup>              |                            ResNet\-101                             |    ICLR 2021     |             85\.9             |      46\.8      |            \-            |
|          **Mask2Former** <sup><a href="#ref30">30</a></sup>           |                              Swin\-L                               |    arXiv 2021    |              \-               |      56\.1      |          83\.3           |
|      **SeMask \+ Mask2Former**<sup><a href="#ref31">31</a></sup>      |                              Swin\-L                               |    arXiv 2021    |              \-               |      57\.5      |          84\.98          |
|            **SwinV2\-G**<sup><a href="#ref32">32</a></sup>            |                                Swin                                |    arXiv 2021    |              \-               |      59\.9      |            \-            |
|              **VOLO**<sup><a href="#ref33">33</a></sup>               |                                VOLO                                |    arXiv 2021    |              \-               |      54\.3      |          84\.3           |
|            **SegFormer**<sup><a href="#ref34">34</a></sup>            | MiT\(Mix Transformer\) Segformer<sup><a href="#ref34">34</a></sup> |    NIPS 2021     |              \-               |      51\.8      |          84\.0           |
|             **UperNet**<sup><a href="#ref35">35</a></sup>             |             Shift\-B<sup><a href="#ref41">39</a></sup>             |    arXiv 2022    |              \-               |      49\.2      |            \-            |
|             **UperNet**<sup><a href="#ref35">35</a></sup>             |            VAN\-Large<sup><a href="#ref42">40</a></sup>            |    arXiv 2022    |              \-               |      50\.1      |            \-            |
|             **UperNet**<sup><a href="#ref35">35</a></sup>             |           ConvNeXt\-XL<sup><a href="#ref43">41</a></sup>           |    arXiv 2022    |              \-               |      54\.0      |            \-            |
| **MCTformer\(Weakly\-Supervised\)**<sup><a href="#ref36">36</a></sup> |                              ResNet38                              |    CVPR 2022     |             71\.6             |       \-        |            \-            |
|   **SIPE\(Weakly\-Supervised\)**<sup><a href="#ref37">37</a></sup>    |                             ResNet101                              |    CVPR 2022     |             69\.7             |       \-        |            \-            |
|   **LAA\(Weakly\-Supervised\)** <sup><a href="#ref38">38</a></sup>    | MiT\(Mix Transformer\) Segformer<sup><a href="#ref34">34</a></sup> |    CVPR 2022     |             66\.3             |       \-        |            \-            |

1. <p name = "ref1">Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.</p>
2. <p name = "ref2">Dai J, He K, Sun J. Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1635-1643.</p>
3. <p name = "ref3">Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1529-1537.</p>
4. <p name = "ref4">Chen L C, Papandreou G, Kokkinos I, et al. Semantic image segmentation with deep convolutional nets and fully connected crfs[J]. arXiv preprint arXiv:1412.7062, 2014.</p>
5. <p name = "ref5">Yu F, Koltun V. Multi-scale context aggregation by dilated convolutions[J]. arXiv preprint arXiv:1511.07122, 2015.</p>
6. <p name = "ref6">Badrinarayanan V, Kendall A, Cipolla R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 39(12): 2481-2495.</p>
7. <p name = "ref7">Chen L C, Papandreou G, Kokkinos I, et al. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 40(4): 834-848.</p>
8. <p name = "ref8">Chen L C, Papandreou G, Schroff F, et al. Rethinking atrous convolution for semantic image segmentation[J]. arXiv preprint arXiv:1706.05587, 2017.</p>
9. <p name = "ref9">Pohlen T, Hermans A, Mathias M, et al. Full-resolution residual networks for semantic segmentation in street scenes[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4151-4160.</p>
10. <p name = "ref10">Zhao H, Shi J, Qi X, et al. Pyramid scene parsing network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2881-2890.</p>
11. <p name = "ref11">Lin G, Milan A, Shen C, et al. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1925-1934.</p>
12. <p name = "ref12">Liang X, Zhou H, Xing E. Dynamic-structured semantic propagation network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 752-761.</p>
13. <p name = "ref13">Mehta S, Rastegari M, Caspi A, et al. Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation[C]//Proceedings of the european conference on computer vision (ECCV). 2018: 552-568.</p>
14. <p name = "ref14">Xiao T, Liu Y, Zhou B, et al. Unified perceptual parsing for scene understanding[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 418-434.</p>
15. <p name = "ref15">Zhao H, Zhang Y, Liu S, et al. Psanet: Point-wise spatial attention network for scene parsing[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 267-283.</p>
16. <p name = "ref16">Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 801-818.</p>
17. <p name = "ref17">Chen L C, Collins M, Zhu Y, et al. Searching for efficient multi-scale architectures for dense image prediction[J]. Advances in neural information processing systems, 2018, 31.</p>
18. <p name = "ref18">Lo S Y, Hang H M, Chan S W, et al. Efficient dense modules of asymmetric convolution for real-time semantic segmentation[M]//Proceedings of the ACM Multimedia Asia. 2019: 1-6.</p>
19. <p name = "ref19">Fu J, Liu J, Wang Y, et al. Stacked deconvolutional network for semantic segmentation[J]. IEEE Transactions on Image Processing, 2019.</p>
20. <p name = "ref20">Ding H, Jiang X, Shuai B, et al. Semantic correlation promoted shape-variant context for segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 8885-8894.</p>
21. <p name = "ref21">Liu C, Chen L C, Schroff F, et al. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 82-92.</p>
22. <p name = "ref22">Mehta S, Rastegari M, Shapiro L, et al. Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 9190-9200.</p>
23. <p name = "ref23">Choi S, Kim J T, Choo J. Cars can't fly up in the sky: Improving urban-scene segmentation via height-driven attention networks[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 9373-9383.</p>
24. <p name = "ref24">Yu C, Wang J, Gao C, et al. Context prior for scene segmentation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 12416-12425.</p>
25. <p name = "ref25">Yuan Y, Chen X, Chen X, et al. Segmentation transformer: Object-contextual representations for semantic segmentation[J]. arXiv preprint arXiv:1909.11065, 2019.</p>
26. <p name = "ref26">Cheng B, Schwing A, Kirillov A. Per-pixel classification is not all you need for semantic segmentation[J]. Advances in Neural Information Processing Systems, 2021, 34: 17864-17875.</p>
27. <p name = "ref27">Zhang X, Xu H, Mo H, et al. Dcnas: Densely connected neural architecture search for semantic image segmentation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 13956-13967.</p>
28. <p name = "ref28">Ranftl R, Bochkovskiy A, Koltun V. Vision transformers for dense prediction[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 12179-12188.</p>
29. <p name = "ref29">Geng Z, Guo M H, Chen H, et al. Is attention better than matrix decomposition?[J]. arXiv preprint arXiv:2109.04553, 2021.</p>
30. <p name = "ref30">Cheng B, Misra I, Schwing A G, et al. Masked-attention mask transformer for universal image segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 1290-1299.</p>
31. <p name = "ref31">Jain J, Singh A, Orlov N, et al. Semask: Semantically masked transformers for semantic segmentation[J]. arXiv preprint arXiv:2112.12782, 2021.</p>
32. <p name = "ref32">Liu Z, Hu H, Lin Y, et al. Swin transformer v2: Scaling up capacity and resolution[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 12009-12019.</p>
33. <p name = "ref33">Yuan L, Hou Q, Jiang Z, et al. Volo: Vision outlooker for visual recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.</p>
34. <p name = "ref34">Xie E, Wang W, Yu Z, et al. SegFormer: Simple and efficient design for semantic segmentation with transformers[J]. Advances in Neural Information Processing Systems, 2021, 34: 12077-12090.</p>
35. <p name = "ref35">Xiao T, Liu Y, Zhou B, et al. Unified perceptual parsing for scene understanding[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 418-434.</p>
36. <p name = "ref36">Xu L, Ouyang W, Bennamoun M, et al. Multi-class token transformer for weakly supervised semantic segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 4310-4319.</p>
37. <p name = "ref37">Chen Q, Yang L, Lai J H, et al. Self-supervised image-specific prototype exploration for weakly supervised semantic segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 4288-4298.</p>
38. <p name = "ref38">Ru L, Zhan Y, Yu B, et al. Learning affinity from attention: end-to-end weakly-supervised semantic segmentation with transformers[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16846-16855.</p>
39. <p name = "ref39">Wang G, Zhao Y, Tang C, et al. When shift operation meets vision transformer: An extremely simple alternative to attention mechanism[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(2): 2423-2430.</p>
40. <p name = "ref40">Guo M H, Lu C Z, Liu Z N, et al. Visual attention network[J]. arXiv preprint arXiv:2202.09741, 2022.</p>
41. <p name = "ref41">Liu Z, Mao H, Wu C Y, et al. A convnet for the 2020s[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 11976-11986.</p>
