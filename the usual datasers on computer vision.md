# 附录

## 检测数据集

| **数据集名**     | **任务类型** | **发表年份** | **数据集地址**  | **引用量** |
|:------------:|:--------:|:--------:|:-------------------------------------------------------------:|:-------:|
| VOC [^1]          | 目标检测     | 2010 | [The PASCAL Visual Object Classes Homepage (ox.ac.uk)](http://host.robots.ox.ac.uk/pascal/VOC/)| 13590    |
| ILSVRC [^2]      | 目标检测     | 2009 | [ImageNet (image-net.org)](https://image-net.org/challenges/LSVRC/index.php)  | 36463      |
| MS-COCO [^3]     | 目标检测     | 2014 | [COCO - Common Objects in Context (cocodataset.org)](https://cocodataset.org/#home)| 22100       |
| KITTI [^4]        | 目标检测     | 2012     | [The KITTI Vision Benchmark Suite (cvlibs.net)](http://www.cvlibs.net/datasets/kitti/)                 | 8415       |
| Open Image V4 [^5]  | 目标检测     | 2020 | [https://storage.googleapis.com/openimages](https://storage.googleapis.com/openimages/web/index.html)                   | 1017       |
| SUN RGB-D  [^6]  | 目标检测     | 2015     | [SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite](https://rgbd.cs.princeton.edu/)        | 1141       |
| INRIA Person [^7] | 行人检测     | 2005 | [http://pascal.inrialpes.fr/data/human/](http://pascal.inrialpes.fr/data/human/)                      | 38666       |
| Caltech Pedestrian Dataset [^8] [^9]    | 行人检测     | 2009 | [Caltech Pedestrian Detection Benchmark](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)                        | 1483      |
|KITTI Road [^10]| 行人检测 | 2013 |<http://www.cvlibs.net/datasets/kitti/eval_road.php> |588|
| CityPersons [^11]  | 行人检测     | 2017 | [Detectron-PYTORCH/data/citypersons](https://github.com/CharlesShang/Detectron-PYTORCH/tree/master/data/citypersons)                            | 494       |
| EuroCity Persons [^12]   | 行人检测     | 2018 | [EuroCity Persons Dataset (tudelft.nl)](https://eurocity-dataset.tudelft.nl/)                         | 47      |
| TJU-DHD [^13]     | 行人检测     | 2020     | [TJU-DHD](https://github.com/tjubiit/TJU-DHD)                                                     | 12      |
| FDDB [^14]        | 面部检测     | 2010 | [http://vis-www.cs.umass.edu/fddb/](http://vis-www.cs.umass.edu/fddb/)             | 1045      |
| AFLW [^15]        | 面部检测     | 2011 | [ICG - AFLW (tugraz.at)](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)       |  975 |
| IJB-A [^16]         | 面部检测     | 2015 | <https://www.nist.gov/programs-projects/face-challenges>        |712|
| WiderFace [^17]    | 面部检测     | 2016 | [WIDER FACE: A Face Detection Benchmark](https://www.nist.gov/programs-projects/face-challenges)             |    1213     |
| WildestFaces [^18] | 面部检测     | 2020 | [Project page for Red Carpet to Fight Club](https://ycbilge.github.io/wildestFaces)                |  0  |
| ICDAR2003 [^19]       | 场景文本检测   | 2003 | [imglab.org > Database / Datasets](http://www.imglab.org/db/index.html)                        | 991   |
| MSRA-TD500 [^20]   | 场景文本检测   | 2012 | [Detecting texts of arbitrary orientations in natural images](http://www.iapr-tc11.org/mediawiki/index.php/MSRA_Text_Detection_500_Database_(MSRA-TD500))   | 777 |
| COCOText [^21]    | 场景文本检测   | 2016 | [COCO-Text V2.0 (bgshih.github.io)](https://bgshih.github.io/cocotext/)         |297|
| Total-Text [^22]  | 场景文本检测   | 2017     | [Total-Text-Dataset: Total Text Dataset](https://github.com/cs-chan/Total-Text-Dataset)            |   239       |
| SCUT-CTW1500 [^23] | 场景文本检测   | 2017     | <https://github.com/Yuliang-Liu/Curve-Text-Detector>                                               |  149    |
| TextSeg [^24]      | 场景文本检测   | 2021     | [Rethinking-Text-Segmentation](https://github.com/SHI-Labs/Rethinking-Text-Segmentation)                                  | 4 |
| GTSDB [^25]        | 交通标志检测   | 2013 | [https://benchmark.ini.rub.de/](https://benchmark.ini.rub.de/)                       | 1110  |
| TT100K [^26]      | 交通标志检测   | 2016 |       <https://cg.cs.tsinghua.edu.cn/traffic-sign/>                  |491               |
| CURE-TSD [^27]     | 交通标志检测   | 2019     | <https://github.com/olivesgatech/CURE-TSD>                      | 30 |
| DoTA [^28]         | 交通标志检测   | 2020     | [Detection-of-Traffic-Anomaly](https://github.com/MoonBlvd/Detection-of-Traffic-Anomaly)                        | 15 |
| ASAYAR  [^29]     | 交通标志检测   | 2020     | <https://vcar.github.io/ASAYAR/>                                             |3      |
| VHR10-NWPU [^30]   | 遥感目标检测   | 2016 | <http://jiong.tea.ac.cn/people/JunweiHan/NWPUVHR10dataset.html> | 897|
| DOTA[^31]          | 遥感目标检测   | 2018 | [DOTA (captain-whu.github.io)](https://captain-whu.github.io/DOTA/index.html)                        | 913       |
| fMoW [^32]      | 遥感目标检测   | 2018     | <https://github.com/fMoW/dataset>                                   | 110     |
| xView  [^33]      | 遥感目标检测   | 2018 | [xView (xviewdataset.org)](http://xviewdataset.org/)                                 |156     |
| INRIA Aerial Image Labeling [^34]       | 遥感目标检测   | 2017     | [<https://project.inria.fr/aerialimagelabeling/>     ](https://project.inria.fr/aerialimagelabeling/)        | 408    |
| LEVIR-CD [^35]    | 遥感目标检测   | 2020     | <https://justchenhao.github.io/LEVIR/>                     | 113  |  

## 检测数据集简介

### 目标检测

**PASCAL Visual Object Classes（VOC）2012**[^1]该数据集包含20个对象类别，包括车辆、飞机、自行车、船、公共汽车、汽车、摩托车、火车等。此数据集中的每个图像都有像素级分割注释、边界框注释和对象类注释。该数据集已被广泛用作目标检测、语义分割和分类任务的基准。PASCAL VOC数据集分为三个子集：1464张用于训练的图像、1449张用于验证的图像和一个专用测试集。  

**ImageNet**[^2]数据集根据WordNet层次结构包含14197122个带注释的图像。自2010年以来，该数据集被用于ImageNet大规模视觉识别挑战赛（ImageNet Large Scale Visual Recognition Challenge, ILSVRC），这个比赛是图像分类和目标检测的主要基准之一。该比赛公开发布的数据集包含一组手动注释的训练图像。还有一组测试图像，保留了手动注释。ILSVRC注释分为两类：（1）针对图像中是否存在对象类的二进制标签的图像级注释。（2） 图像中对象实例的边界框和类标签的对象级注释

**MS COCO**(Microsoft Common Objects in Context)[^3]是一个关于大规模的对象检测、分割、关键点检测和字幕数据集，其中包含80个类别。数据集由328K个图像组成。MS COCO数据集的第一个版本于2014年发布。它包含164K个图像，分为训练集（83K）、验证集（41K）和测试集（41K）。2015年，发布了81K图像的附加测试集，包括所有原始测试图像和40K新增图像。根据社区反馈，2017年，训练/验证划分从83K/41K更改为118K/5K。新拆分使用相同的图像和注释。2017年测试集是2015年测试集41K图像的子集。此外，2017版包含一个新的123K图像的未注释数据集。

**KITTI**[^4]是用于移动机器人和自动驾驶的最流行的数据集之一。它包括用各种传感器(高分辨率RGB、灰度立体摄像机和3D激光扫描仪)采集到的各种交通场景。尽管数据集较受欢迎，但它本身并不包含语义分割的基本事实。然而，许多研究人员已经对数据集的某些部分进行了手动注释，以满足他们的需要。

**Open Image**[^5]包含V4和V6两个版本， V4提供了跨多个维度的大规模功能：19.8k概念的30.1M图像级别标签，600个对象类的15.4M边界框，以及涉及57个类的375k视觉关系注释。特别是对于目标检测，提供的边界框比下一个最大的数据集（1.9M图像上的15.4M框）多15倍。这些图像内容通常是包含多个对象的复杂场景（每个图像平均有8个带注释的对象）。它们之间的视觉关系被注释，支持视觉关系检测，这是一项需要结构化推理的新兴任务。V6由900万个训练图像、41620个验证样本和125456个测试样本组成。它是一个部分注释的数据集，包含9600个可训练类。

**SUN RGB-D**[^6]包含10335个来自现实室内场景的RGB-D图像。每个RGB图像都有相应的深度和分割图。标记的对象类别多达700个。训练集和测试集分别包含5285和5050幅图像。

### 行人检测

**INRIA Person**[^7]是用于行人检测的人员图像数据集。它包括614人的训练集和288人的测试集。  

**Caltech Pedestrian Dataset**[^8] [^9]为大约时长10个小时的视频，视频尺寸为640x480，帧数为30Hz。主要拍摄日常交通行驶在城市环境中的车辆。其中包含大约25万帧，标注了35万个边界框和2300个行人。注释中包括边界框和详细遮挡标签之间的时间对应关系。

**KITTI Road**[^10]是道路和车道估计基准，由289个训练图像和290个测试图像组成。它包含三个不同类别的道路场景

**CityPersons**[^11]数据集是Cityscapes\cite{cordts2015cityscapes}的一个子集，它只包含行人注释。有2975张图像用于训练，500张和1575张图像用于验证和测试。图像中行人的平均数量为7，并提供了可见区域和全身注释。

**EuroCity Persons**[^12]数据集提供了大量关于城市交通场景中行人、自行车骑手的多样、准确和详细的注释。该数据集的图像是在12个欧洲国家的31个城市的一辆移动车辆上收集的。在超过47300张图像中，人工标记了超过238200关于人的实例，EuroCity Persons比之前用于基准测试的Person数据集大近一个数量级。此外，该数据集还包含大量的人员定位注释（超过211200条）。

**TJU-DHD**[^13]是用于目标检测和行人检测的高分辨率数据集。该数据集共包含115354张高分辨率图像（52\%的图像分辨率为1624×1200像素，48\%的图像分辨率至少为2560×1440像素）和709330个标记对象，数据之间大小和外观差异较大。

### 面部图像

**FDDB**[^14]数据集是自然场景下的人脸标记集合。它总共包含5171个人脸注释，图像分辨率也不尽相同（例如363x450和229x410）。该数据集包含一系列挑战，包括复杂的姿势角度、失焦面部和低分辨率。包括灰度和彩色图像。

**AFLW(Annotated Facial Landmarks in the Wild)**[^15]是从Flickr收集的注释面部图像的大规模集合，展示了各种各样的外观（例如不同的姿势、表情、种族、年龄、性别）以及不同的背景。总共约25K个面部被注释，每个图像最多有21个标注。

**IJB-A(IARPA Janus Benchmark A)**[^16]数据集收集了在姿态、光照、表情、分辨率和遮挡等方面变化很大的面部图像，增加人脸识别任务的更多挑战。IJB-A由包含500个目标的5712张图片和2085个视频构建而成，平均每个目标有11.4张图片和4.2个视频。

**WiderFace**[^17]数据集包含32203张图像和393703张标签，在比例、姿势和遮挡方面具有高度的可变性。数据库分为训练集（40\%）、验证集（10\%）和测试集（50\%）。此外，图像根据检测的难度分为简单、中等、困难三个级别。在线提供训练集和验证集的图像和注释，测试集的注释不公开，结果直接发送到数据库服务器以接收精度召回曲线。

**WildestFaces**[^18]用于研究各种不利条件下的跨域识别，数据集分为视频和净图，视频为平均帧数为25fps被分割成最多持续10秒的镜头。由64位被选中的演员的410个视频中的2186个镜头组成，总计64242帧。净图中包含64位演员的8069张图像。

### 场景文本

**ICDAR2003**[^19]数据集是用于场景文本识别的数据集。共包含507幅自然场景图像（包括258幅训练图像和249幅测试图像）。图像在字符级别进行注释，可以从图像中裁剪字符和单词。

**MSRA-TD500**[^20]是一个包含300张训练图像和200张测试图像的文本检测数据集。文本区域在句子层面上是任意方向标注的。与其他数据集不同的是，它同时包含了英文和中文文本。

**COCO-Text**[^21]数据集是用于文本检测和识别的数据集。它是基于MS-COCO数据集，其中包含具有复杂的日常场景的图像。COCO-Text数据集包含非文本图像、可读文本图像和不可读文本图像。总共有22184个训练图像和7026个验证图像，每幅图像至少有一个清晰的文本实例。

**Total-Text**[^22]是一个文本检测数据集，由1,555幅图像组成，包含各种文本类型，包括水平文本、多方向文本和弯曲文本实例。训练集和测试集分别有1255张和300张图像。

**SCUT-CTW1500**[^23]数据集包含1500张图像：1000张用于训练，500张用于测试。特别是，它提供10751个裁剪文本实例图像，包括3530个曲线文本。这些图像是从互联网、图像库（如Google Open image）或手机摄像头中手动获取的。数据集包含大量横向和多方向文本。

**TextSeg**[^24]是一个大规模精细注释和多用途文本检测和分割数据集，收集场景并设计具有六种注释类型的文本：文字和字符边界多边形、遮罩和转置。

### 交通标志

**GTSRB**[^25]包含43类交通标志，分为39209张训练图像和12630张测试图像。这些图像具有不同的光照条件和丰富的背景。

**Tsinghua-Tencent 100K**[^26]数据集是从10万张腾讯街景全景图中选出的大型交通标志基准，该数据集提供100000个图像，包含30000个交通标志实例。这些图像涵盖了照度和天气条件的巨大变化。基准中的每个交通标志都使用类别标签、其边界框和像素掩码进行注释。

**CURE-TSD**[^27]数据集是在比利时街道拍摄的实时交通视频,由2989个视频组成，其中包括8967600帧标记的图片以及648186个标记的交通信号指示。

**DoTA(Detection of Traffic Anomaly)**[^28]数据集包含4,677个视频与时间，空间，分类的注释。

**ASAYAR**[^29]是第一个公共数据集专门用于拉丁语(法语)和阿拉伯语场景文本检测在高速公路面板。它包含了超过1800张注释良好的图像。该数据集是收集自摩洛哥公路，并已手工注释。ASAYAR数据可用于开发和评估不同语言的交通标志检测和法语或阿拉伯语文本检测。

### 航拍图像

**VHR10-NWPU**[^30]是由西北工业大学标注的航天遥感目标检测数据集，共有800张图像，其中包含目标的650张，背景图像150张，目标包括：飞机、舰船、油罐、棒球场、网球场、篮球场、田径场、港口、桥梁、车辆10个类别。

**DOTA(Dataset for Object deTection in Aerial Images)**[^31]数据集包含2806张遥感图像，共有15类188282个实例，其中每个实例都由一个四边形界框标注，定点按顺时针顺序进行排列。

**fMoW(Functional Map of the World)**[^32]是一个旨在启发机器学习模型的开发的数据集，能够从卫星图像的时间序列和丰富的元数据特征中预测建筑物和土地利用的功能用途。

**xView**[^33]是最大的公开可用的遥感图像数据集之一。它包含来自世界各地复杂场景的图像，使用边界框进行注释。它包含了来自60个不同类的超过1M的对象实例。

**INRIA Aerial Image Labeling**[^34]数据集由360个5000×5000px的RGB贴图组成，空间分辨率为30cm/px，覆盖全球10个城市。一半的城市用于训练，并与公共地面的建筑脚印的真相。数据集的其余部分仅用于评估隐藏的地面真相。结合公共区域图像和公共区域官方建筑足迹构建数据集。

**LEVIR-CD**[^35]由637对1024 × 1024像素大小的高分辨率(VHR, 0.5m/pixel)谷歌地球图像块成对组成。时间跨度为5 ~ 14年，期间土地利用特别是建设用地发生了显著变化。LEVIR-CD涵盖了各种类型的建筑，如别墅住宅、高层公寓、小型车库和大型仓库。其中完整注释的LEVIR-CD总共包含31,333个单独的变更构建实例。

## 分割

| **数据集名**                                                     | **任务类型** | **发表年份** |                **数据集地址**                 | **引用量** |
|:--------------------------------------------------------:|:----:|:----:|:-------------------------------------------------------------------------------------------:|:---:|
| Berkeley Segmentation Dataset [^36]                           | 自然场景 | 2001 | <https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/>            |   7113  |
| VOC [^1]                                               | 自然场景 | 2009 |  [The PASCAL Visual Object Classes Homepage (ox.ac.uk)](http://host.robots.ox.ac.uk/pascal/VOC/)          |  13590   |
| Stanford Background Dataset [^37]                              | 自然场景 | 2009 | <http://dags.stanford.edu/projects/scenedataset.html>               | 828    |
| Microsoft COCO [^3]                                         | 自然场景 | 2015 | [COCO - Common Objects in Context (cocodataset.org)](https://cocodataset.org/#home)                  | 22100    |
| MIT Scene parsing data(ADE20K) [^38]                          | 自然场景 | 2016 | <https://groups.csail.mit.edu/vision/datasets/ADE20K/>        | 1319    |
| Semantic Boundaries Dataset [^39]                              | 自然场景 | 2011 | <http://home.bharathh.info/pubs/codes/SBD/download.html>         |  1156   |
| MSRC-12 [^40]                                                  | 自然场景 | 2006 | <https://pgram.com/dataset/msrc-12-kinect-gesture-data-set/>                                 |  463   |
| Densely Annotated Video Segmentation(DAVIS) [^41]              | 视频分割 | 2017 | <https://davischallenge.org/>        |  1113   |
| YouTube-Video object Segmentation [^42]                       | 视频分割 | 2018 | <https://youtube-vos.org/>       |  182   |
| Cambridge-driving Labeled Video Database (CamVid) [^43]       | 自动驾驶 | 2009 | <http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/>         |   1058  |
| Cityscapes: Semantic Urban Scene Understanding   [^44]       | 自动驾驶 | 2016 | <https://www.cityscapes-dataset.com/dataset-overview/>    |  6692   |
| Mapillary Vistas Dataset [^45]                                | 自动驾驶 | 2017 | <https://www.mapillary.com/dataset/vistas?lat=20&lng=0&z=1.5&pKey=301072681638536>       |   693  |
| SYNTHIA: Synthetic collection of Imagery and Annotations [^46] | 自动驾驶 | 2016 | <https://synthia-dataset.net/>             |  1148   |
| SemanticKITTI [^47]                                           | 自动驾驶 | 2019 | <http://www.semantic-kitti.org/> |   8439  |
| Berkeley Deep Drive  [^48]                                    | 自动驾驶 | 2018 | [GitHub - gy20073/BDD_Driving_Model](https://github.com/gy20073/BDD_Driving_Model/)        |   683  |
| Indian Driving Dataset  [^49]                                 | 自动驾驶 | 2018 |     <http://idd.insaan.iiit.ac.in/>           |  112   |
| Inria Aerial Image Labeling Dataset [^50]                     | 航拍图像 | 2017 | <https://project.inria.fr/aerialimagelabeling/>               |  411   |
| DeepGlobe  [^51]                                           | 航拍图像 | 2018 | <http://deepglobe.org/>     |   388  |
| DRIVE:Digital Retinal Images for Vessel Extraction [^52]       | 医学影像 | 2004 | <https://drive.grand-challenge.org/>     |   3350  |
| BRATS: Brain Tumor Segmentation   [^53]                       | 医学影像 | 2015 | <https://www.smir.ch/BRATS/Start2015>                                                         |   3094  |
| Breast Cancer Semantic Segmentation (BCSS) dataset [^54]      | 医学影像 | 2019 |<https://github.com/PathologyDataScience/BCSS>                   |   57  |
| Medical Segmentation Decathlon  [^55]                         | 医学影像 | 2019 | [<http://medicaldecathlon.com/>                 ](http://medicaldecathlon.com/)                                               |   339  |
| BIMCV COVID-19 [^56]                                          | 医学影像 | 2020 | <https://github.com/BIMCV-CSUSP/BIMCV-COVID-19>                                               |  75   |
| PadChest [^57]                                                | 医学影像 | 2019 | <https://github.com/auriml/Rx-thorax-automatic-captioning>                      |   186  |
| MSRA10k Salient Object Database [^58]                          | 显著检测 | 2017 | <https://mmcheng.net/msra10k/>                                                                |  1087   |
| ECSSD: Extended Complex Scene Saliency Dataset [^59]           | 显著检测 | 2015 | [Extended Complex Scene Saliency Dataset (cuhk.edu.hk)](https://www.cse.cuhk.edu.hk/leojia/projects/hsaliency/dataset.html)     |   391  |
| PASCAL-S DATASET [^60]                                        | 显著检测 | 2014 | <https://cbs.ic.gatech.edu/salobj/>                                                           |  1118   |
| SALICON (Salicency in Context)   [^61]                        | 显著检测 | 2015 | <http://salicon.net/>                                                                               |  542   |
| LFSD(Light Field Saliency Database) [^62]                     | 显著检测 | 2014 | <https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/>         |  340   |
| KAIST Scene Text Database [^63]                               | 场景文本 | 2010 | <http://www.iapr-tc11.org/mediawiki/index.php/KAIST_Scene_Text_Database>                      |  40   |
| COCO-Text [^21]                                               | 场景文本 | 2016 | <https://bgshih.github.io/cocotext/>                                                          | 297    |
| SVT: Street View Text Dataset  [^64]                          | 场景文本 | 2010 | <http://vision.ucsd.edu/~kai/svt/>                                                       | 528    |

### 自然场景

**BSD(Berkeley Segmentation Dataset)**[^36]是常用的图像去噪和超分辨率处理的数据集。其子集BSD100是一个由Martin等人提出的具有100个测试图像的类图像数据集。该数据集由自然图像和特定对象（如植物、人、食物等）组成。此外，BSD100是伯克利分割数据集BSD300的测试集。

**Stanford background**[^37]斯坦福背景数据集包含715个RGB图像和相应的标签图像。图像大小约为240×320像素，像素分为八个不同类别。

**ADE20K**[^38]语义分割数据集包含超过20K个以场景为中心的图像，这些图像使用像素级对象和对象部分标签进行了详尽的注释。共有150个语义类别，包括天空、道路、草地等内容，以及人、车、床等离散对象。

**SBD(Semantic Boundaries Dataset)**[^39]是用于预测对象边界上像素的数据集。该数据集由来自PASCAL VOC 2011挑战赛trainval集合的11318张图像组成，分为8498张训练图像和2820张测试图像。此数据集具有对象实例边界，具有精确的地物/地面遮罩，这些遮罩还标记有20个Pascal VOC类之一。

**MSRC-12**[^40]数据集包括一系列以身体部位位置表示的人体动作，以及系统需要识别的相关手势。数据集包括594个序列和719359帧，大约6小时40分钟，收集了30个人做的12个手势，总共包含6244个手势实例。运动文件包含使用Kinect姿态估计管道估计的20个关节的轨迹，其中身体姿势以30Hz的采样率被捕捉，关节位置的精确度约为2厘米。

### 视频分割

**DAVIS (Densely Annotated VIdeo Segmentation)**[^41]是在480p和1080p两种不同分辨率下的高质量、高分辨率的密集标注视频分割数据集。有50个视频序列，3455个像素级密集标注帧。30个2079帧的视频用于训练，以及20个1376帧的视频用于验证。

**YouTube-VOS 2018 (Youtube Video Object Segmentation)**[^42]是一个视频对象分割数据集，包含4,453个视频，其中3471个用于训练，474个用于验证，508个用于测试。训练和验证视频每5/6帧有像素级基准注解。它还包含实例分割注释。它拥有超过7800个独特的对象，190k高质量的手工注释，时长超过340分钟

### 自动驾驶

**CamVid (Cambridge-driving Labeled Video Database)**[^43]是一个道路/驾驶场景理解数据库，最初是作为五个视频序列与960×720分辨率摄像机安装在汽车的仪表盘上捕获的。对这些序列进行采样(其中4个以1帧/秒的速度采样，1个以15帧/秒的速度采样)，共计701帧。这些图片被手工标注了32个类:建筑物、墙体、树木、植被、围栏、人行道、停车区、桥梁、标志、杂文、交通灯、天空、隧道、拱门、道路、车道标识(驾驶)、车道标识(非驾驶)、动物、行人、儿童、自行车、摩托车、轿车、SUV/皮卡/卡车、火车、和其他移动的物体

**Cityscapes**[^44]是一个大型的基于语义理解的城市街景数据库。它分为8类(平面、人、车辆、建筑、物体、自然、天空和虚空)的30个类提供语义、实例和密集的像素标注。该数据集由大约5000幅精细标注图像和20000幅粗标注图像组成。数据采集了50个城市几个月时间的街道场景数据。它最初是通过视频录制的，所有帧是手动选择的，具有以下特点:大量的动态对象，变化的场景布局，和随机的背景。

**Mapillary Vistas Dataset**[^45]是一个大规模的街道级场景数据集，包含25,000张标注为66/124个对象类别的高分辨率图像，其中37/70个类别是实例特定的标签。通过使用多边形来描述单个对象，使用密集和细粒度方式注释。

**SYNTHIA**[^46]是一个合成数据集，由9400个由虚拟城市渲染的多视点视觉图像组成，并为13个类提供像素级语义注释，每帧的分辨率为1280 × 960。

**SemanticKITTI**[^47]是用于点云语义分割的大型户外场景数据集。它源于KITTI Vision Odometry Benchmark，并通过所采用的汽车激光雷达的360视场进行密集点标注。该数据集由22个序列组成。总的来说，该数据集提供了23201个点云数据用于训练，20351个点云数据用于测试。

**Berkeley DeepDrive Video**[^48]由真实驾驶视频和GPS/IMU数据组成的数据集。BDDV的数据集包含了不同的驾驶场景，包括美国几个主要城市的城市、高速公路、城镇和农村地区。

**IDD(Indian Driving Dataset)**[^49]是一个用于非结构化环境下的道路场景理解的数据集，用于自动驾驶的语义分割和目标检测。它包含了10004张图像，在印度道路上的182个驾驶序列中收集了34个分类，并进行了精细的注释。

### 航拍图像

**IAILD(Inria Aerial Image Labeling Dataset)**[^50]数据集图像覆盖810 km²(405 km²用于训练，405 km²用于测试)。图像空间分辨率为0.3 m的航空正射校正彩色图像。数据按照建筑与非建筑分为两个语义类（只公开训练集数据）。

**DeepGlobe**[^51]是来自于DeepGlobe 2018 Satellite Image Understanding Challenge中的数据集，包括分割，检测和分类任务的卫星图像三个公开基准挑战。与计算机视觉领域的其他挑战(如DAVIS和COCO)类似，DeepGlobe提出了三种数据集和相应的评估方法，并与2018年CVPR的专门研讨会共同举办了三场比赛。

### 医学影像

**DRIVE (Digital Retinal Images for Vessel Extraction)**[^52]是一个视网膜血管分割的数据集。它由40张JPEG彩色眼底图像组成;其中病理异常数据7例。这些图像来自荷兰的一个糖尿病视网膜病变筛查项目，每个图像分辨率为584*565像素,20幅图像作为训练集，20幅作为测试集。

**BraTS 2015**[^53]数据集为脑肿瘤图像分割数据集。它包括220个高级别胶质瘤(High Grade Gliomas,HGG)和54个低级别胶质瘤(Low Grade Gliomas,LGG)的核磁共振成像。四种MRI模式分别为T1、T1c、T2和T2FLAIR。肿瘤内部包含四种情况，即水肿、增强瘤、非增强瘤和坏死。

**BCSS(Breast Cancer Semantic Segmentation)**[^54]数据集包含了超过20000个来自癌症基因组图谱(The Cancer Genome Atlas, TCGA)的乳腺癌图像的组织区域分割标注。通过病理学家、病理住院医师和医学院学生的共同努力，使用数字幻灯片档案对这个大规模数据集进行了注释，为组织分割生成高度精确的机器学习模型。

**Medical Segmentation Decathlon**[^55]是医学图像分割数据集的集合。它包含了总共2,633张三维图像，这些图像来自多个感兴趣的解剖、多种形态和多种病理来源。具体来说，它包含以下身体器官或部位的数据:大脑、心脏、肝脏、海马体、前列腺、肺、胰腺、肝血管、脾脏和结肠。

**BIMCV COVID-19+**[^56]是一个包含COVID-19患者胸部X射线图像CXR (CR, DX)和CT图像、放射学检查结果、病理学结果、聚合酶链反应(polymerase chain reaction，PCR)、免疫球蛋白G 结果(immunoglobulin G，IgG)和免疫球蛋白M (immunoglobulin M，IgM)诊断抗体检测的数据集，影像学报告来自巴伦西亚地区医学影像库(Valencia Region Image Bank ，BIMCV)。这些发现被映射到统一的医学语言系统(Unified Medical Language System，UMLS)术语上，它们覆盖了广泛的胸部实体，与之前数据集中注释的少量实体数量形成对比。图像以高分辨率存储，实体以医学影像数据结构(Medical Imaging Data Structure， MIDS)格式的解剖标签进行定位。此外，由放射专家团队对23幅图像进行了注释，包括对放射学结果的语义分割注释。同时还提供了广泛的辅助信息，包括患者的人口统计学信息、投影类型和成像研究的获取参数等。

**PadChest**[^57]是一个标记的大规模、高分辨率胸部x射线数据集，用于自动探索医学图像及其相关报告。该数据集包括从67000名患者获得的160000多张图像，这些图像来自西班牙圣胡安医院的放射科在2009年到2017年的报告，涵盖了六种不同的体位视图以及关于图像采集和患者人口统计的附加信息。这些报告用174个不同的放射学检查结果、19个鉴别诊断和104个解剖位置标记为分层分类法，并映射到标准统一医学语言系统（Unified Medical Language，UMLS）术语。

### 显著检测

**MSRA10K**[^58]是一个用于显著对象检测的数据集，其中包含10000幅图像，这些图像具有像素级显著性标签。原始MRSA数据库根据3-9个用户提供的边界框提供显著的对象注释。

**ECSSD(Extended Complex Scene Saliency Dataset)**[^59]显著性数据集由复杂场景组成，呈现真实世界场景中常见的纹理和结构。ECSSD包含1000张复杂的真实图像和相应的基准显著性图像，这些图像注释是根据五名人类参与者的标记进行统一评估。

**PASCAL-S**[^60]是用于显著对象检测的数据集，由PASCAL VOC 2010验证集的850幅图像组成，其中场景中有多个显著对象。

**SALICON(The SALIency in CONtext)**[^61]数据集包含10000张训练图像、5000张验证图像和5000张显著性测试图像。该数据集是通过注释来自MS-COCO的图像中的显著性创建的。地面真实显著性注释包括从鼠标轨迹生成的固定。为了提高数据质量，排除了局部密度低的孤立固定。训练和验证集提供了地面真相，包含以下数据字段：图像、分辨率和凝视。测试数据仅包含图像和分辨率字段。

**LFSD(Light Field Saliency Database)**[^62]数据集包含100个360×360空间分辨率的光场，为每个光场提供一个粗略的焦点堆栈和一个全聚焦图像。该数据集中的图像通常有一个突出的前景对象和一个具有良好颜色对比度的背景。

### 场景文本

**KAIST Scene Text**[^63]数据集包含在不同环境中捕获的3000幅图像，包括在不同光照条件下的室外和室内场景(晴天、夜晚、强人工灯光等)。照片分别使用高分辨率的数码相机或低分辨率的手机相机拍摄的，所有图像统一大小为640x480。
  
**SVT: Street View Text Dataset**数据集来源于谷歌Street View。这些数据中的图像文本具有很高的随机性，通常分辨率较低。[^64] [^65]中有更多关于该数据集的信息。

## 分类

| **数据集名**          | **任务类型** | **发表年份** | **描述**                                                      | **引用量** |
|:-----------------:|:--------:|:--------:|:-----------------------------------------------------------:|:-------:|
| CIFAR [^66]            | 图像分类     | 2009     | <https://www.cs.toronto.edu/~kriz/cifar.html>                 |    14607     |
| ImageNet  [^2]        | 图像分类     | 2009     | [ImageNet (image-net.org)](https://image-net.org/challenges/LSVRC/index.php)                                   |     36560    |
| MNIST [^67]         | 图像分类     | 1998     | <http://yann.lecun.com/exdb/mnist/>                           |  43809       |
| SVHN [^68]             | 图像分类     | 2011     | <http://ufldl.stanford.edu/housenumbers/>                     |     3503    |
| CelebA [^69]           | 图像分类     | 2015     | <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>                               |   4842      |
| Oxford 102 Flower [^70] | 图像分类     | 2008     | <https://www.robots.ox.ac.uk/~vgg/data/flowers/102/>          |    1948     |
| CUB-200-2011 [^71]     | 图像分类     | 2011     | <http://www.vision.caltech.edu/visipedia/CUB-200-2011.html>   |   2267      |
| Office-31 [^72]        | 图像分类     | 2010     | <https://www.cc.gatech.edu/~judy/domainadapt/>                |   2172     |
| Food-101 [^73]         | 图像分类     | 2014     | <https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/> |        861 |
| Stanford Cars [^74]    | 图像分类     | 2013     | <https://ai.stanford.edu/~jkrause/cars/car_dataset.html>      |    1743     |
| Clothing1M [^75]       | 图像分类     | 2015     | <https://github.com/Cysu/noisy_label>                         |     705    |
| MIT Indoor Scenes [^76]      | 场景识别     | 2009     |      <https://www.kaggle.com/itsahmad/indoor-scenes-cvpr-2019>                                                       |   1526      |
| SUN397  [^77]          | 场景识别     | 2010     | <https://vision.princeton.edu/projects/2010/SUN/>             |     2726    |
| Places  [^78]          | 场景识别     | 2017     | <http://places.csail.mit.edu/>                                |   2142      |
| HSD [^79]              | 场景识别     | 2019     | <https://usa.honda-ri.com/hsd>                                |    7     |
| SUN RGB-D  [^6]       | 场景识别     | 2015     | <https://rgbd.cs.princeton.edu/>                              |    1141     |
| RESISC45  [^80]        | 场景识别     | 2017     | <http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html>  |  1144       |
| ADVANCE [^81]          | 场景识别     | 2020     | <https://akchen.github.io/ADVANCE-DATASET/>                   |     8    |
| YUP++ [^82]            | 视频分类     | 2017     | <https://vision.eecs.yorku.ca/research/dynamic-scenes/>       |     76    |
| YouTube-8M [^83]       | 视频分类     | 2016     | <https://research.google.com/youtube8m/>                      |    985     |
| Kinetics [^84]          | 动作分类     | 2017     | <https://deepmind.com/research/open-source/kinetics>          |   1956      |
| Charades [^85]         | 动作分类     | 2016     | <http://vuchallenge.org/charades.html>                        |    705     |
| THUMOS14 [^86]         | 动作分类     | 2017     | <http://crcv.ucf.edu/THUMOS14/home.html>                      |     268    |
| Something V2 [^87]     | 动作分类     | 2016     | <https://20bn.com/datasets/something-something>               | 522 |

### 图像分类

**CIFAR**[^66]数据集包括两个子集，分别是CIFAR-10(Canadian Institute for Advanced Research, 10 classes)和CIFAR-100(Canadian Institute for Advanced Research, 100 classes)。这两个数据集均由60000张32*32分辨率的彩色微小图像图像构成。CIFAR-10图像包含10个类别:飞机、汽车(但不是卡车或皮卡)、鸟、猫、鹿、狗、青蛙、马、船和卡车(但不是皮卡)。每个类有6000张图片，其中5000张训练图片和1000张测试图片。CIFAR-100中的100个类被分组为20个超类，每个类有600张图片。每个图像都带有一个“精细”标签(它所属的类)和一个“粗糙”标签(它所属的超类)。每个类有500个训练图像和100个测试图像。

**MNIST**[^67]数据库(Modified National Institute of Standards and Technology database)是大量手写数字的集合。它有一个包含60,000张训练集和10,000张测试集。它分别由NIST特殊数据库3(由美国人口普查局雇员编写的数字)和特殊数据库1(由高中生编写的数字)的子集，其中包含手写数字的灰度图像。数字经过标准化处理并在固定大小的图像中居中。NIST的原始灰度图像经过尺寸归一化处理为20x20像素。结果图像包含灰度级别作为归一化算法使用的抗锯齿技术的结果。通过计算像素的质心，将图像置于28x28图像的中心，并平移图像，使该点置于28x28域的中心。

**SVHN (Street View House Numbers)**[^68]是一个数字分类基准数据集，包含600000张从RGB车牌图像中裁剪的数字信息(0到9)，大小为32×32。裁剪后的图像以感兴趣的数字居中，但仍存在其他干扰。SVHN分为三组数据:训练集、测试集和另外一组包括53万张图像的验证集，可用于帮助模型训练。

**CelebFaces Attributes**[^69]数据集包含了来自10177位名人的202,599张大小为178×218的面部图像，每张图像都标注了40个二进制标签，表示头发颜色、性别和年龄等面部属性。

**Oxford 102 Flower**[^70]是由102个花卉类别组成的图像分类数据集。被选为花的花通常出现在英国。每一类由40到258张图片组成。图像数据在比例、姿势和光线等方面具有可变性。此外，还有一些图像类内差距较大，类间差距较小。

**CUB-200-2011 (Caltech-UCSD Birds-200-2011)**[^71]是用于细粒度可视化分类任务最常用的数据集。它包含了属于鸟类200个子类的11788张图像，其中5994张用于训练，5794张用于测试。每个图像都有详细的注释:1个子类别标签，15个部分位置，312个二进制属性和1个边界框。文本信息来自Reed等人。他们通过收集细粒度的自然语言描述来扩展CUB-200-2011数据集。每个图像收集10个单句描述。通过亚马逊机械土耳其人(AMT)平台收集自然语言描述，要求至少10个单词，不包含任何子类别和动作信息。

**Office-31**[^72]数据集包含三个领域的31个对象类别:Amazon、DSLR和Webcam。数据集中的31个类别包括在办公室设置中的对象，如键盘、文件柜和笔记本电脑。Amazon类中每个子类平均包含90张图片，总共包含2817张图片。因为这些图片是来此在线商家的网站，所以它们具有干净的背景和统一的规模。DSLR域包含498张低噪声高分辨率图像(428848张)。每个类别有5个对象，每个物体平均从不同角度拍摄3次。对于网络摄像机，795张低分辨率(640×480)的图像显示明显的噪声和颜色以及白平衡伪影.

**food-101**[^73]数据集包含101个食品类别，每个类别有750个训练图像和250个测试图像，总共有101k张图像。测试图像不具有ground-truth，同时训练集包含噪声数据。

**Stanford Cars**[^74]数据集由196种汽车组成，共16,185张图片，这些图片均为汽车背影。数据被分成大约50-50的训练/测试分割，有8,144张训练图像和8,041张测试图像。类别通常是在制造、型号、年份级别。图片是360240。

**Clothing1M**[^75]数据集包含了14个类的1M服装图片。它是一个有噪声标签的数据集，因为数据是从几个在线购物网站收集的，包括许多错误标签的样本。该数据集还包含50k、14k和10k图像，分别用于训练、验证和测试过程。

### 场景识别

**MIT Indoor Scenes (MIT67)**[^76]涵盖了广泛的室内场景，如商店、公共空间。MIT67包括来自67个类别的15620幅室内场景图像，每个类别约有100幅图像，数据集的最小分辨率为200-200像素。由于这个数据集中的对象普遍存在相似性，对图像进行分类具有挑战性。在训练集和测试集中，每个类分别有80和20张图像。

**SUN397(The Scene UNderstanding)**[^77]数据库包含899个类别和130,519张图像。有397个采样良好的类别，以评估许多最先进的算法的场景识别。

**Places**[^78]数据集用于场景识别，包含超过250万张图像，覆盖超过205个场景类别，每个类别包含超过5000张图像。

**HSD (Honda Scenes Dataset)**[^79]发布了一个带注解的数据集，以实现动态场景分类，包括在旧金山湾区收集的80小时不同的高质量驾驶视频数据剪辑。该数据集包括道路位置、道路类型、天气和路面状况的时间注释。

**SUN RGB-D**[^6]数据集包含10335张真实的室内场景RGB-D图像。每个RGB图像都有相应的深度图和分割图。有多达700个对象类别被标记。训练集和测试集分别包含5285和5050幅图像。

**RESISC45**[^80]数据集是用于遥感图像场景分类的数据集。它包含31,500张图片，45个场景类，每个类包含700张图片。

**ADVANCE (AuDio Visual Aerial sceNe reCognition datasEt)**[^81]是一个全新的多模态学习数据集，旨在探索音频和传统视觉信息对场景识别的贡献。该汇总数据集包含5075对经过地理标记的航拍图像和声音，分为机场、运动场、海滩、桥梁、农田、森林、草地、港口、湖泊、果园、居民区、灌木林、火车站等13个场景类。

### 视频分类

**YUP++ (YUP++ Dynamic Scenes dataset)**[^82]是一个新的具有挑战性的动态场景视频数据库，其大小是以前可用视频的两倍多。这个数据集被明确地分成两个大小相等的子集，其中包含有和没有摄像机运动的视频，以便系统地研究这个变量是如何与场景本身的定义动态进行交互的。

**YouTube-8M**[^83]数据集是一个大型视频数据集，包含700多万段视频，标注系统标注了4716个类。该数据集由三个部分组成:训练集、验证集和测试集。在训练集中，每个类包含至少100个训练视频。这些视频的特征是由最先进的流行的预先训练模型提取，并发布给公众使用。每个视频包含音频和视觉形态。根据视频的视觉信息，将视频分为体育、游戏、艺术、体育、娱乐等24个主题。

### 动作分类

**Kinetics(Kinetics Human Action Video Dataset)**[^84]数据集是一个大规模的、高质量的视频人体动作识别数据集。该数据集包含约50万个视频剪辑，涵盖600个人类动作，每个动作类至少有600个视频剪辑。每个视频剪辑持续大约10秒，并被标记为一个单独的动作类。这些视频是从YouTube上收集的。

**Charades**[^85]数据集由9848个平均长度为30秒的日常室内活动视频组成，涉及15种室内场景中46个物体类的交互，包含30个动词词汇，导致157个动作类。这个数据集中的每个视频都由多个自由文本描述、动作标签、动作间隔和交互对象的类别进行注解。267个不同的用户观察同一个句子，其中包括固定词汇中的物体和动作，他们录制了一段视频来表演这个句子。总的来说，数据集包含157个动作类的66500个时态注释，46个对象类的41104个标签，以及视频的27847个文本描述。在标准的分割中，有7986个训练视频和1863个验证视频。

**THUMOS14**[^86]数据集是一个大规模的视频数据集，包括来自20个类的1010个验证视频和1574个测试视频。在所有的视频中，验证集和测试集中分别有220个和212个时间标注的视频。

**20BN-SOMETHING-SOMETHING V2**[^87]数据集是一个标记视频剪辑的大集合，展示了人类对日常物体执行预定义的基本动作。该数据集是由大量人群工作者创建的。它允许机器学习模型对发生在物理世界中的基本行为进行细粒度的理解。它共包含220,847个视频，其中训练集168,913个，验证集24,777个，测试集27,157个。

[^1]: Everingham M, Van Gool L, Williams C K I, et al. The pascal visual object classes (voc) challenge[J]. International journal of computer vision, 2010, 88(2): 303-338.
[^2]: Deng J, Dong W, Socher R, et al. Imagenet: A large-scale hierarchical image database[C]//2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009: 248-255.
[^3]: Lin T Y, Maire M, Belongie S, et al. Microsoft coco: Common objects in context[C]//European conference on computer vision. Springer, Cham, 2014: 740-755.
[^4]: Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? the kitti vision benchmark suite[C]//2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012: 3354-3361.
[^5]: Kuznetsova A, Rom H, Alldrin N, et al. The open images dataset v4[J]. International Journal of Computer Vision, 2020, 128(7): 1956-1981.
[^6]: Song S, Lichtenberg S P, Xiao J. Sun rgb-d: A rgb-d scene understanding benchmark suite[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 567-576.
[^7]: Dalal N, Triggs B. Histograms of oriented gradients for human detection[C]//2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05). Ieee, 2005, 1: 886-893.
[^8]: Dollar P, Wojek C, Schiele B, et al. Pedestrian detection: An evaluation of the state of the art[J]. IEEE transactions on pattern analysis and machine intelligence, 2011, 34(4): 743-761.
[^9]: Dollár P, Wojek C, Schiele B, et al. Pedestrian detection: A benchmark[C]//2009 IEEE conference on computer vision and pattern recognition. IEEE, 2009: 304-311.
[^10]:Fritsch J, Kuehnl T, Geiger A. A new performance measure and evaluation benchmark for road detection algorithms[C]//16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013). IEEE, 2013: 1693-1700.
[^11]: Zhang S, Benenson R, Schiele B. Citypersons: A diverse dataset for pedestrian detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3213-3221.
[^12]: Braun M, Krebs S, Flohr F, et al. The eurocity persons dataset: A novel benchmark for object detection[J]. arXiv preprint arXiv:1805.07193, 2018.
[^13]: Pang Y, Cao J, Li Y, et al. TJU-DHD: A diverse high-resolution dataset for object detection[J]. IEEE Transactions on Image Processing, 2020, 30: 207-219.
[^14]: Jain V, Learned-Miller E. Fddb: A benchmark for face detection in unconstrained settings[R]. UMass Amherst technical report, 2010.
[^15]: Koestinger M, Wohlhart P, Roth P M, et al. Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization[C]//2011 IEEE international conference on computer vision workshops (ICCV workshops). IEEE, 2011: 2144-2151.
[^16]: Klare B F, Klein B, Taborsky E, et al. Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1931-1939.
[^17]: Yang S, Luo P, Loy C C, et al. Wider face: A face detection benchmark[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 5525-5533.
[^18]: Bilge Y C, Yucel M K, Cinbis R G, et al. Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021: 3358-3369.
[^19]: Lucas S M, Panaretos A, Sosa L, et al. ICDAR 2003 robust reading competitions: entries, results, and future directions[J]. International Journal of Document Analysis and Recognition (IJDAR), 2005, 7(2): 105-122.
[^20]: Yao C, Bai X, Liu W, et al. Detecting texts of arbitrary orientations in natural images[C]//2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012: 1083-1090.
[^21]: Veit A, Matera T, Neumann L, et al. Coco-text: Dataset and benchmark for text detection and recognition in natural images[J]. arXiv preprint arXiv:1601.07140, 2016.
[^22]: Ch'ng C K, Chan C S. Total-text: A comprehensive dataset for scene text detection and recognition[C]//2017 14th IAPR international conference on document analysis and recognition (ICDAR). IEEE, 2017, 1: 935-942.
[^23]: Yuliang L, Lianwen J, Shuaitao Z, et al. Detecting curve text in the wild: New dataset and new solution[J]. arXiv preprint arXiv:1712.02170, 2017.
[^24]: Xu X, Zhang Z, Wang Z, et al. Rethinking text segmentation: A novel dataset and a text-specific refinement approach[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 12045-12055.
[^25]: Stallkamp J, Schlipsing M, Salmen J, et al. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition[J]. Neural networks, 2012, 32: 323-332.
[^26]: Zhu Z, Liang D, Zhang S, et al. Traffic-sign detection and classification in the wild[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2110-2118.
[^27]: Temel D, Chen M H, AlRegib G. Traffic sign detection under challenging conditions: A deeper look into performance variations and spectral characteristics[J]. IEEE Transactions on Intelligent Transportation Systems, 2019, 21(9): 3663-3673.
[^28]: Yao Y, Wang X, Xu M, et al. When, where, and what? a new dataset for anomaly detection in driving videos[J]. arXiv preprint arXiv:2004.03044, 2020.
[^29]: Akallouch M, Boujemaa K S, Bouhoute A, et al. ASAYAR: A dataset for Arabic-Latin scene text localization in highway traffic panels[J]. IEEE Transactions on Intelligent Transportation Systems, 2020.
[^30]: Cheng G, Han J. A survey on object detection in optical remote sensing images[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2016, 117: 11-28.
[^31]: Xia G S, Bai X, Ding J, et al. DOTA: A large-scale dataset for object detection in aerial images[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3974-3983.
[^32]:Christie G, Fendley N, Wilson J, et al. Functional map of the world[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6172-6180.
[^33]:Lam D, Kuzma R, McGee K, et al. xview: Objects in context in overhead imagery[J]. arXiv preprint arXiv:1802.07856, 2018.
[^34]: Maggiori E, Tarabalka Y, Charpiat G, et al. Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark[C]//2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, 2017: 3226-3229.
[^35]: Chen H, Shi Z. A spatial-temporal attention-based method and a new dataset for remote sensing image change detection[J]. Remote Sensing, 2020, 12(10): 1662.
[^36]: Martin D, Fowlkes C, Tal D, et al. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001. IEEE, 2001, 2: 416-423.
[^37]: Gould S, Fulton R, Koller D. Decomposing a scene into geometric and semantically consistent regions[C]//2009 IEEE 12th international conference on computer vision. IEEE, 2009: 1-8.
[^38]: Zhou B, Zhao H, Puig X, et al. Scene parsing through ade20k dataset[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 633-641.
[^39]: Hariharan B, Arbeláez P, Bourdev L, et al. Semantic contours from inverse detectors[C]//2011 international conference on computer vision. IEEE, 2011: 991-998.
[^40]: Fothergill S, Mentis H, Kohli P, et al. Instructing people for training gestural interactive systems[C]//Proceedings of the SIGCHI conference on human factors in computing systems. 2012: 1737-1746.
[^41]: Perazzi F, Pont-Tuset J, McWilliams B, et al. A benchmark dataset and evaluation methodology for video object segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 724-732.
[^42]: Xu N, Yang L, Fan Y, et al. Youtube-vos: A large-scale video object segmentation benchmark[J]. arXiv preprint arXiv:1809.03327, 2018.
[^43]: Brostow G J, Fauqueur J, Cipolla R. Semantic object classes in video: A high-definition ground truth database[J]. Pattern Recognition Letters, 2009, 30(2): 88-97.
[^44]: Cordts M, Omran M, Ramos S, et al. The cityscapes dataset for semantic urban scene understanding[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 3213-3223.
[^45]: Neuhold G, Ollmann T, Rota Bulo S, et al. The mapillary vistas dataset for semantic understanding of street scenes[C]//Proceedings of the IEEE international conference on computer vision. 2017: 4990-4999.
[^46]: Ros G, Sellart L, Materzynska J, et al. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 3234-3243.
[^47]: Geiger A, Lenz P, Urtasun R. Are we ready for autonomous driving? the kitti vision benchmark suite[C]//2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012: 3354-3361.
[^48]: Xu H, Gao Y, Yu F, et al. End-to-end learning of driving models from large-scale video datasets[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2174-2182.
[^49]: Varma G, Subramanian A, Namboodiri A, et al. IDD: A dataset for exploring problems of autonomous navigation in unconstrained environments[C]//2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019: 1743-1751.
[^50]: Maggiori E, Tarabalka Y, Charpiat G, et al. Can semantic labeling methods generalize to any city? the inria aerial image labeling benchmark[C]//2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, 2017: 3226-3229.
[^51]: Demir I, Koperski K, Lindenbaum D, et al. Deepglobe 2018: A challenge to parse the earth through satellite images[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018: 172-181.
[^52]: Staal J, Abràmoff M D, Niemeijer M, et al. Ridge-based vessel segmentation in color images of the retina[J]. IEEE transactions on medical imaging, 2004, 23(4): 501-509.
[^53]: Menze B H, Jakab A, Bauer S, et al. The multimodal brain tumor image segmentation benchmark [BRATS](J). IEEE transactions on medical imaging, 2014, 34(10): 1993-2024.
[^54]: Amgad M, Elfandy H, Hussein H, et al. Structured crowdsourcing enables convolutional segmentation of histology images[J]. Bioinformatics, 2019, 35(18): 3461-3467.
[^55]: Simpson A L, Antonelli M, Bakas S, et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms[J]. arXiv preprint arXiv:1902.09063, 2019.
[^56]: Vayá M I, Saborit J M, Montell J A, et al. Bimcv covid-19+: a large annotated dataset of rx and ct images from covid-19 patients[J]. arXiv preprint arXiv:2006.01174, 2020.
[^57]: Bustos A, Pertusa A, Salinas J M, et al. Padchest: A large chest x-ray image dataset with multi-label annotated reports[J]. Medical image analysis, 2020, 66: 101797.
[^58]: Hou Q, Cheng M M, Hu X, et al. Deeply supervised salient object detection with short connections[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3203-3212.
[^59]: Shi J, Yan Q, Xu L, et al. Hierarchical image saliency detection on extended CSSD[J]. IEEE transactions on pattern analysis and machine intelligence, 2015, 38(4): 717-729.
[^60]: Li Y, Hou X, Koch C, et al. The secrets of salient object segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 280-287.
[^61]: Jiang M, Huang S, Duan J, et al. Salicon: Saliency in context[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1072-1080.
[^62]: Li N, Ye J, Ji Y, et al. Saliency detection on light field[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014: 2806-2813.
[^63]: Jung J, Lee S H, Cho M S, et al. Touch TT: Scene text extractor using touchscreen interface[J]. ETRI Journal, 2011, 33(1): 78-88.
[^64]: Wang K, Belongie S. Word spotting in the wild[C]//European conference on computer vision. Springer, Berlin, Heidelberg, 2010: 591-604.
[^65]: Wang K, Babenko B, Belongie S. End-to-end scene text recognition[C]//2011 International conference on computer vision. IEEE, 2011: 1457-1464.
[^66]: Krizhevsky A, Hinton G. Learning multiple layers of features from tiny images[J]. 2009.
[^67]: LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.
[^68]: Netzer Y, Wang T, Coates A, et al. Reading digits in natural images with unsupervised feature learning[J]. 2011.
[^69]: Liu Z, Luo P, Wang X, et al. Deep learning face attributes in the wild[C]//Proceedings of the IEEE international conference on computer vision. 2015: 3730-3738.
[^70]: Nilsback M E, Zisserman A. Automated flower classification over a large number of classes[C]//2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing. IEEE, 2008: 722-729.
[^71]: Wah C, Branson S, Welinder P, et al. The caltech-ucsd birds-200-2011 dataset[J]. 2011.
[^72]: Saenko K, Kulis B, Fritz M, et al. Adapting visual category models to new domains[C]//European conference on computer vision. Springer, Berlin, Heidelberg, 2010: 213-226.
[^73]: Bossard L, Guillaumin M, Gool L V. Food-101–mining discriminative components with random forests[C]//European conference on computer vision. Springer, Cham, 2014: 446-461.
[^74]: Krause J, Stark M, Deng J, et al. 3d object representations for fine-grained categorization[C]//Proceedings of the IEEE international conference on computer vision workshops. 2013: 554-561.
[^75]: Xiao T, Xia T, Yang Y, et al. Learning from massive noisy labeled data for image classification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 2691-2699.
[^76]: Quattoni A, Torralba A. Recognizing indoor scenes[C]//2009 IEEE conference on computer vision and pattern recognition. IEEE, 2009: 413-420.
[^77]: Xiao J, Hays J, Ehinger K A, et al. Sun database: Large-scale scene recognition from abbey to zoo[C]//2010 IEEE computer society conference on computer vision and pattern recognition. IEEE, 2010: 3485-3492.
[^78]: Zhou B, Lapedriza A, Khosla A, et al. Places: A 10 million image database for scene recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 40(6): 1452-1464.
[^79]: Narayanan A, Dwivedi I, Dariush B. Dynamic traffic scene classification with space-time coherence[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019: 5629-5635.
[^80]: Cheng G, Han J, Lu X. Remote sensing image scene classification: Benchmark and state of the art[J]. Proceedings of the IEEE, 2017, 105(10): 1865-1883.
[^81]: Hu D, Li X, Mou L, et al. Cross-task transfer for geotagged audiovisual aerial scene recognition[C]//European Conference on Computer Vision. Springer, Cham, 2020: 68-84.
[^82]: Feichtenhofer C, Pinz A, Wildes R P. Temporal residual networks for dynamic scene recognition[C]//Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2017: 4728-4737.
[^83]: Abu-El-Haija S, Kothari N, Lee J, et al. Youtube-8m: A large-scale video classification benchmark[J]. arXiv preprint arXiv:1609.08675, 2016.
[^84]: Kay W, Carreira J, Simonyan K, et al. The kinetics human action video dataset[J]. arXiv preprint arXiv:1705.06950, 2017.
[^85]: Sigurdsson G A, Varol G, Wang X, et al. Hollywood in homes: Crowdsourcing data collection for activity understanding[C]//European Conference on Computer Vision. Springer, Cham, 2016: 510-526.
[^86]: Idrees H, Zamir A R, Jiang Y G, et al. The THUMOS challenge on action recognition for videos “in the wild”[J]. Computer Vision and Image Understanding, 2017, 155: 1-23.
[^87]: Goyal R, Ebrahimi Kahou S, Michalski V, et al. The" something something" video database for learning and evaluating visual common sense[C]//Proceedings of the IEEE international conference on computer vision. 2017: 5842-5850.
